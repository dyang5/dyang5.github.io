<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>REU-CAAR, Week 7 | David Yang</title>
    <meta name="author" content="David  Yang">
    <meta name="description" content="Week 7 of my Summer 2023 experience @ REU-CAAR.">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img//avatar.jpg">
    
    <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
    <link rel="canonical" href="http://localhost:4000/blog/2023/REUCAAR-week7/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">
    <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script>
    <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/">David Yang</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">CV</a>
              </li>
              <li class="nav-item dropdown ">
                <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">more</a>
                <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown">
                  <a class="dropdown-item" href="/swarthmore/">swarthmore</a>
                </div>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- _layouts/post.html -->

<div class="post">

  <header class="post-header">
    <h1 class="post-title">REU-CAAR, Week 7</h1>
    <p class="post-meta">July 23, 2023</p>
    <p class="post-tags">
      <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>
        ·  
        <a href="/blog/tag/research">
          <i class="fas fa-hashtag fa-sm"></i> research</a>  
          <a href="/blog/tag/cs">
          <i class="fas fa-hashtag fa-sm"></i> cs</a>  
          
        ·  
        <a href="/blog/category/caar">
          <i class="fas fa-tag fa-sm"></i> caar</a>  
          

    </p>
  </header>

  <article class="post-content">
    
    <div id="markdown-content">
      <h4 id="day-43-july-17th"><u>Day 43 (July 17th)</u></h4>

<p>Week 7, Day 1</p>

<ul>
  <li>
    <p>8:30 - 9:00 AM: Wow. It’s week 7 already, which is crazy. Time is dwindling – I hope we get some cool research results to present in just under three weeks. Instead of getting out for my Monday morning swim, I played it safe and waited for AG, the grad lead for REU-CAAR, to bring all the isolating students COVID Test Kits (which is quite nice of him). I tested Negative and so that meant I could finally get back to work at IRB. I broughout out my mask just to be safe as well.</p>
  </li>
  <li>
    <p>10:00 AM - 12:00 PM: To start off the research week, I wanted to work on testing the results from the generated images (from our diffusion models) on the DeepFace classifier. After some experimentation with prompting and comparing the results to work from my other group members, I generated 15 images for each of the four types of prompts (portrait/group images of doctors/firefighters) and proceeded to hand-label the portrait images. I then used the image to classification pipeline that I had written from previous weeks, modified some of the code, and ran it on the generated images. To analyze the results, I would need to hand-label the images (since they are generated and are not part of a labeled dataset). After some discussion with my group, I ended up doing this for the portrait images before heading to lunch.</p>
  </li>
  <li>
    <p>12:00 - 1:30 PM: We had another one of our weekly Monday REU lunch talks. Today Bill gave a talk on <a href="https://www.cs.umd.edu/~gasarch/REU/hatstalk.pdf" rel="external nofollow noopener" target="_blank">The Hat Problem</a>. I had seen this problem before but did not remember the strategy (and I also had not seen the many extensions proposed by Bill); overall, I found the talk quite enjoyable and it was great to think mathematically/problem-solve for the first time in a while. I definitely want to ask Bill whether I can present a version of this to professors at Swarthmore, or even present it as part of the IMC (Interactive Math Colloquium) Club I run, since I very much enjoyed this talk!</p>
  </li>
  <li>
    <p>1:30 - 4:00 PM: I quite enjoyed Bill’s Hat Talk so I spent some time afterwards discussing the proofs (especially the infinite hat, 2 colors and infinite hat, at most 1 error cases) with AF and SS (XG, our final group member, is going to be working remotely this week). We talked a bit about the <a href="https://en.wikipedia.org/wiki/Axiom_of_choice" rel="external nofollow noopener" target="_blank">Axiom of Choice</a>, our thoughts on it, as well as some resulting paradoxes including Banach–Tarski (the Axiom of Choice is used in one of the proofs discussed by Bill). Afterwards, I ended up discussing the general research plan (what everyone was working on, potential limitations of our research and what we can do to address them, and what we can do next to get a solid research project before the REU ends) with my group members AF and SS. I think this was really productive – at times, I feel like I’ve only been working on a small part of the pipeline, but it’s important to realize that the general fairness process we are trying to build relies on many different small but essential parts. AF and I also tried to reason through mathematical foundation behind a Latent Diffusion Model paper SS was trying to read through, to some success. I didn’t get too much done outside of this group discussion, but I feel like having these sorts of discussions is really necessary to get everyone aligned and on the same page and to build a solid working dynamic, so I was quite happy with that.</p>
  </li>
</ul>

<p>After research, I ended up relaxing a bit before heading out to “play badminton” with ZF, AG, and SS. It turns out there were only 3 badminton racquets, and since I didn’t want them to have to rotate, I ended up playing Racquetball on my own for 30ish minutes. Afterwards, I waited around and watched them and others play badminton. This ended up taking much longer than anticipated, which I was okay with since I mainly was on my phone after a while, but it meant that we had to jog to CFA before it closed at 7. Obviously, then, since we ordered right before closing, the food wasn’t as good as normal. Oh well.</p>

<p>Outside of research, I’ve been waiting for my monitor to arrive (it was supposed to arrive yesterday but the delivery didn’t go through), and it hasn’t arrived yet today, either, which kind of sucks. I’m really looking forward to a better setup to hopefully allow me to be more productive and work easier in my room. I’ve also been waiting to hear from the <a href="https://ymc.osu.edu/" rel="external nofollow noopener" target="_blank">Young Mathematicians Conference</a>, a math conference I applied to using my work from last year, so fingers crossed on that front!</p>

<h4 id="day-44-july-18th"><u>Day 44 (July 18th)</u></h4>

<p>Week 7, Day 2</p>

<ul>
  <li>
    <p>8:30 - 9:00 AM: I had planned to exercise outside yesterday (swim or run) and in the morning today, but we hit another wave of bad air quality (I think this is the third one in this REU program alone, with 130+ AQI, meaning the Air Quality is unhealthy for outdoor activity). When I woke up at 6:30 AM to check on the AQI, it was still Unhealthy, so I set my alarms for 9 to get an extra hour of sleep.</p>
  </li>
  <li>
    <p>10:00 AM - 12:00 PM: Over the last few days, I’ve been focused on trying to get good diffusion model output as well as trying to test the DeepFace classifier on the generated output. For the latter part, I have everything I need to do this (I would need to write a bit more code to do so but it wouldn’t take too long), but I have been putting it off. My reasoning for this is twofold: many of the generated group images have terrible image quality (faces are deformed), making labeling and comparing to the results from the classifier difficult/not useful. Furthermore, even for the single images (one person in an image), the hand-labeling of a person’s race is very subjective and thus it’s difficult to accurately analyze the classifier. As a result, I’ve moved away from this and have thought more about how we can get fair image generation. One of our current projects, led by AF, is Domain Translation, which involves masking a certain part of an image (e.g. one person) and using a Blended Latent Diffusion model to “translate” that person’s gender or race. For example, one can mask a man and say “change this person to a woman.” Ideally, this person would look very similar characteristic wise as to what the original person is wearing, since the model tries to “blend” the result into the rest of the image. Through some initial experiments, this has been unsuccessful/difficult. As a result, I’ve been looking into some work that follows a <a href="https://arxiv.org/pdf/2301.12247.pdf" rel="external nofollow noopener" target="_blank">Semantic Guidance</a> paper, where given an image and a concept, the concept is added/suppressed in the image while preserving the other characteristics of an image.</p>
  </li>
  <li>
    <p>1:00 - 4:00 PM: In the morning, I read a bit about the paper and the general process before starting to implement the code from the <a href="https://colab.research.google.com/github/ml-research/semantic-image-editing/blob/main/examples/SemanticGuidance.ipynb#scrollTo=4e6e6724" rel="external nofollow noopener" target="_blank">Semantic Guidance Colab</a> locally. I got this working and started looking at generating group images and how guidance on gender/race would affect the image. Tomorrow, I want to try to combine our existing domain translation approach (where a certain part of the image is masked) and the Semantic Guidance approach. My (maybe optimistic) hope is to be able to mask a certain part of the image, guide that part of the image, and “stitch” the resulting parts together to lead to a fair outcome, as this will preserve general background characteristics in a generated image while making the image more fair. We closed the day by talking a bit as a group, and we somehow ended up working on some math problems from China’s famous Gaokao examination (which was fun) as well.</p>
  </li>
</ul>

<p>After research, I headed to the gym to play basketball with PM, MA, AB, SS, and DS before joining AG and ZS at the pool afterwards. We mainly messed around at the pool (there was so many people and the lanes were all packed), which included me and ZS attempting to teach SS how to dive into a pool as well as going on the slide/diving boards. I also briefly swam a few laps near the end, when lanes started to free up. I got home back around 8 PM, warmed up some leftover pizza, did some CodeForces, and that was that.</p>

<p>For some non-research related content, I had to cancel my monitor shipment from Amazon today, since it has not been delivered despite remaining at the Amazon facilities. This kind of sucks since I was hoping to work on a monitor for the remainder of the REU (and I will hand it off to Sherry post research for her to use in industry). On the plus side, I found out today I was accepted as a Report Presenter at this year’s YMC (Young Mathematicians Conference), which I am quite happy about! I guess maybe not getting a monitor for now is a small price to pay for getting accepted into YMC. :)</p>

<h4 id="day-45-july-19th"><u>Day 45 (July 19th)</u></h4>

<p>Week 7, Day 3</p>

<ul>
  <li>
    <p>8:30 - 9:00 AM: AQI is back to normal and I’m out of isolation so there are no excuses for not going for a morning swim now! I, admittedly reluctantly, got out of bed and over to the pool for a quick 25 minute, 40 lap swim. It’s good to get back to lap swimming and hopefully I’ll get back into the groove of things quickly.</p>
  </li>
  <li>
    <p>10:00 AM - 12:00 PM: In the morning, I experimented a bit more with the Semantic Guidance Pipeline from diffusers, specifically the prompt manipulation which leads to a different generated image. A minor yet useful adjustment I made was displaying multiple images in a grid-like fashion, so I can compare the results before/after each guidance stage and compare it to the original image output. It seems like there’s a difference between guiding “male” and reverse-guiding “female,” so we may need to think about which one we want if there is an abundance of one gender (for example, guiding “male” may generate a few men in places where the original image had no person – this may not be desirable).</p>
  </li>
  <li>
    <p>1:00 - 4:00 PM: I started to test the Semantic Guidance pipeline a bit more on group images. I think I will need to tinker around more with the other hyperparameters (including the number of editing steps, guidance strength, edit momentum, etc.) to get a handle of the parameters we would want to use to lead to more fair image generation. Adding the reverse prompting of “deformed, blurry, …” and other negative prompts I used in stable diffusion experimentation led to slightly better quality images most of the time, but when I used these prompts without the additional guidance of a gender term, the model started generating almost exclusively people with darker skin and doctor hats on… which is weird. I’ll continue experimenting with this to see why this is the case. Also, I have been thinking that we will need to use Semantic Guidance in combination with a mask, but I don’t think this is the case anymore – especially if this alone can lead to more fair image generation for groups, which would itself be novel; the masking approach is more of a postprocessing than the one I am currently trying.</p>
  </li>
</ul>

<p>After research, I hung around in our work room to get a little bit more work done, type up this, and then head down for our second Board Game night of the REU. I started by joining a mini game of SET before starting a new game of Olympus with DS and Bill Gasarch, the program director. After a bit of a learning curve and Bill teaching us how to play, we successfully completed a game, with Bill finishing first with 51 points, me finishing second with 48, and DS finishing in last with 31 (which was hilarious since he was joking about how he wanted to finish ahead of me). In between, we paused for some catering from a nearby Indian place, which was much tastier than I had expected. I closed out the night by briefly playing some Poker and some Codenames with some graduate students as well as SS, AZ, DS, MA, LH, and ML.</p>

<p>There is no meeting with our advisor this week since she is heading to Honolulu for <a href="https://icml.cc/" rel="external nofollow noopener" target="_blank">ICML (International Conference on Machine Learning)</a>, but we plan to update her on Slack with our weekly progress.</p>

<h4 id="day-46-july-20th"><u>Day 46 (July 20th)</u></h4>

<p>Week 7, Day 4</p>

<ul>
  <li>
    <p>8:30 - 9:00 AM: Once again, I got out of bed reluctantly to head out for a morning swim. I have been sleeping closer to midnight these last few days and might have to sleep a bit earlier to be more energized for these swims… Anyways, I ended up going for 30 minutes and 50+ laps, which is great pace-wise especially considering I’m just coming back from a break due to isolation and bad air quality.</p>
  </li>
  <li>
    <p>10:00 AM - 12:00 PM: On Thursday mornings, we usually have grad meetings with my advisor’s group, but everyone is out for the previously mentioned ICML conference; as a result, the next two grad meetings are canceled. This means a bit more work time on Thursday mornings, and so today I continued to work a bit more on Semantic Guidance. I am hoping to solve individual fairness first (similar to other papers, like Fair Diffusion) but apply my own touch for the approach. As a result, I also spent some of the morning looking through the Fair Diffusion paper’s code and thinking about where I can extend their procedure/improve on it.</p>
  </li>
  <li>
    <p>1:30 - 4:00 PM: In the afternoon session, I generated and modified a few images using SEGA (Semantic Guidance) but mainly focused on writing an update for my mentor (our updates to her will take the place of our meeting this week). Here are my thoughts, split into 1) limitations of our project/approaches so far and 2) what I’ve been working on this week.</p>
  </li>
</ul>

<blockquote>
  <p>As we talked about in last week’s group meeting, we have been trying to think about some of the limitations of our current approaches. It is difficult to say how we can deal with them, but we are trying to answer them/keep them in mind as we work on our different approaches:
    &lt;li&gt;For the generative process, we find that stable diffusion generated images yield deformed/blurry faces for group images. Ideally, we want these to be higher quality/more human like (as this might make our classifiers more accurate resulting in higher fairness).&lt;/li&gt;
    &lt;li&gt;In our general pipeline, we acknowledge that our gender/race classifier (e.g. DeepFace) may itself be biased/faulty, yet it is a part of our general process for fair image generation.&lt;/li&gt;
    &lt;li&gt;We have not figured out how to separate fairness across different concepts (doctors and nurses). For example, if a generated image has 5 male doctors and 5 female nurses, our RL approach may deem this as a “fair image” when it is technically not.&lt;/li&gt;</p>
</blockquote>

<blockquote>
  <p>On the personal side, here’s a bit more about what I’m now working on: last week I tried experimenting with negative prompting in stable diffusion and completed prompt-to-image and image-to-classification pipelines. This week I’ve been focusing on experimenting with SEGA (“Semantic Guidance”), a technique that uses semantic guidance to “guide” diffusion model output. This is similar to the domain translator approach Amy has been working on, but the domain translator modifies the generated diffusion image, whereas SEGA guides the diffusion process by modifying vectors in the semantic space. My current goals for SEGA include getting fair image outputs over a batch for photos with just one person. To extend previous work, I hope to work more with the editing and guidance hyperparameters (for example, in group fairness, I may want to adjust the guidance scales based on the confidence of the race classifiers) as well as use gender and race classifiers to guide the editing process. In comparison, previous papers like Fair Diffusion support prompts of just one format “a photo of the face of a [occupation]” and the guidance is done manually by the user. Though I plan to start by working to solve individual person fairness, I hope to extend these ideas to group fairness as well.</p>
</blockquote>

<ul>
  <li>4:00 - 5:30 PM: I went to a talk by Professor Victor Yakovneko (Physics Professor at UMD) on “The Statistical Mechanics of Money.” The talk was about how the Boltzmann-Pareto distribution (which is typically used to describe the physics of objects in a system) can be similarly applied to monetary concepts such as income/tax flow. I thought the talk itself was pretty interesting, but it ended up going quite a bit over, which was bad since I really had to use the restroom.</li>
</ul>

<p>After research activities, I went straight to the gym to play basketball. MA and I arrived first, to no available balls and no available courts. After some clever finagling and asking around, we ended up with both a half-court to play on and a basketball, and so we got a game going with SK, SS, MA, PM, DS, and later, two other students. For dinner, SS let me have one of her Shin Ramen packs, and I had that along with some leftovers from PE from two days before. Afterwards, I showered up, confirmed my participation in YMC this year (yay!), and did some CodeForces to close out the night.</p>

<h4 id="day-47-july-21st"><u>Day 47 (July 21st)</u></h4>

<p>Final Work Day of Week 7!</p>

<ul>
  <li>
    <p>8:30 - 9:00 AM: Snoozed my alarm today. My reasoning is that the lane lines are always shuffled on Friday mornings, which cuts my normal swimming time in half. Instead, I took the extra hour of sleep and planned to swim after research…</p>
  </li>
  <li>
    <p>10:00 - 12:00 PM: As I wrote previously, my goal has now shifted to solidfying the Semantic Guidance approach for individual fairness. Today, my main goal was to tinker with the hyperparameters to understand how each hyperparameter affects the generated diffusion image. In the morning, I worked on trying different prompts and thought about how I could incorporate classifier results into the hyperparameters (as a preliminary idea, I think I could make each race an editing prompt and set their relative strengths to 1 minus the relative confidence predictions for each race).</p>
  </li>
  <li>
    <p>1:00 - 4:00 PM: In the afternoon, I worked more on the hyperparameter tuning approach. I was planning to use GridSearchCV, which is an exhaustive hyperparameter approach I learned in Swat’s ML class that tries every combination of hyperparameters, but since I also want to visualize the results, I wanted to try something a bit easier. I ended up setting up the hyperparameter tests using two for loops, where the outer for loop represents the hyperparameter being changed (1 of 6) and the inner for loop represents the value it is changed to (1 of 4), with all other hyperparameters constant. I got some really weird results (some of the generated images look like noise + a bird even though the prompt is “a photo of the face of a doctor”) that I will need to parse more through.</p>
  </li>
</ul>

<p>After some serious morning planning, a bunch of us from the REU group went to the movie theaters to catch the premiere of Barbie &amp; Oppenheimer, with the group leaning towards wanting to watch Barbie… (I would have preferred Oppenheimer). I ended up taking an Uber with SS and AF, and PM drove MA, DS, JS, AZ to the 4:45 movie. I had no expectations for the Barbie movie but I thought it was relatively well-written, and the ending message was pretty nice. Afterwards, I went to The Spot Mini with MA, SS, AF, and AZ (bad idea bc my stomach hurt very bad afterwards) before grabbing my computer monitor (it finally arrived) from the nearby Amazon locker and heading home. 7 weeks are gone already… time flies.</p>

<h4 id="day-48-july-22nd"><u>Day 48 (July 22nd)</u></h4>

<p>Week 7, Day 6: No More Spot Mini</p>

<p>I had a terrible end to the night yesterday with a bad stomachache from whatever I ate at The Spot Mini. I haven’t felt great after eating there before, but my stomach hasn’t felt that bad in a while. It was really hurting yesterday night, and so I went to sleep relatively early – it felt better as I was falling asleep. The stomachache definitely didn’t disappear since I woke up at 12:30 AM with a bad pain in my stomach. After texting AF and SS to ask if they had any Tylenol (so I could get some rest of the night), AF clutched up and gave me some Pepto Bismal, which is supposed to be for upset stomachaches. I felt a lot better after taking it and fell asleep shortly after near 1.</p>

<p>My stomach felt much better when I woke up in the morning. I had initially planned to play tennis with AF and SS at 11 but wasn’t sure due to the complications yesterday night. Luckily, since I felt much better, I went out to play with AF (we normally play on Sundays but we have a REU hiking activity tomorrow which should be fun). Afterwards, I grabbed some PE, ran into SS and JS in line, and SS, AF, and I ate lunch together at the Union. I took a quick break (and nap) before I went out later in the afternoon to get some swimming laps in. The swimming workout itself was great – I had to share a lane for the first bit, but a lane freed up and I got in 80 laps to make up for a missed workout yesterday. After the pool, I went to pick up a package and headed to Subway to grab a sandwich to-go for dinner.</p>

<p>In the evening, I drafted some emails to go out to professors and opened up my new monitor – all in all, definitely quite an eventful day! Glad to get through it with a stomach that feels good, for sure.</p>

<h4 id="day-48-july-22nd-1"><u>Day 48 (July 22nd)</u></h4>

<p>End of Week 7: Trip to Great Falls</p>

<p>I woke up early today (7:30 AM) for a group REU to Great Falls, a nearby hiking trail with a waterfall. I left with the first group (AG, SK, JS, and AF), and we played some Alphabet Games and Contact! on the way to get the conversations flowing on the way there. We met up with ZS, SS, and LH, and proceeded to look at the waterfalls/hike for about two to three hours. On the way back, we stopped at Silver Diner, a really nice diner spot, for lunch. I switched cars along the way, heading back with ZS, SS, and LH, and when I made it back, I went straight to the UMD pool to get some laps in.</p>

<p>After showering up after a workout, I proceeded to head out with ZS and SS for Qu Ramen, which was quite good. We stopped for some Kung Fu Tea before heading back, where I worked on some CodeForces and drafted some more emails. All in all, it was quite an eventful day and a great close to the seventh week of the REU! It was a lot of fun to interact with more of the REU on the hike &amp; lunch, and I hope to continue to do that with the remaining three weeks.</p>

    </div>
  </article>


  
    
    <br>
    <hr>
    <br>
    <ul class="list-disc pl-8"></ul>

    <!-- Adds related posts to the end of an article -->
    <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2>
    <p class="mb-2">Here are some more articles you might like to read next:</p>
  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/YMC-writeup/">Young Mathematicians Conference</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/REUCAAR-week10/">REU-CAAR, Week 10</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/REUCAAR-week9/">REU-CAAR, Week 9</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/REUCAAR-week8/">REU-CAAR, Week 8</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/REUCAAR-week6/">REU-CAAR, Week 6</a>
  </li>

</div>

      
    </div>

    <!-- Footer -->    <!-- 
    <footer class="fixed-bottom">
      <div class="container mt-0">
        &copy; Copyright 2023 David  Yang. Powered by <a href="https://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank">Unsplash</a>.

      </div>
    </footer> -->

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script>
  <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script>
  <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
